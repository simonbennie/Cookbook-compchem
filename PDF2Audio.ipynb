{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrL_MhvI1xPn"
      },
      "source": [
        "# PDF to Audio Converter\n",
        "\n",
        "This code can be used to convert PDFs into audio podcasts, lectures, summaries, and more. It uses OpenAI's GPT models for text generation and text-to-speech conversion.\n",
        "\n",
        "Source: [https://github.com/lamm-mit/PDF2Audio](https://github.com/lamm-mit/PDF2Audio)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrPccs_M1qZ9"
      },
      "outputs": [],
      "source": [
        "!pip install loguru gradio promptic pydantic pypdf tenacity openai -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEduHdSD1fE-"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import concurrent.futures as cf\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from tempfile import NamedTemporaryFile\n",
        "from typing import List, Literal\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "from loguru import logger\n",
        "from openai import OpenAI\n",
        "from promptic import llm\n",
        "from pydantic import BaseModel, ValidationError\n",
        "from pypdf import PdfReader\n",
        "from tenacity import retry, retry_if_exception_type\n",
        "\n",
        "import re\n",
        "\n",
        "def read_readme():\n",
        "    readme_path = Path(\"README.md\")\n",
        "    if readme_path.exists():\n",
        "        with open(readme_path, \"r\") as file:\n",
        "            content = file.read()\n",
        "            # Use regex to remove metadata enclosed in -- ... --\n",
        "            content = re.sub(r'--.*?--', '', content, flags=re.DOTALL)\n",
        "            return content\n",
        "    else:\n",
        "        return \"README.md not found. Please check the repository for more information.\"\n",
        "\n",
        "# Define multiple sets of instruction templates\n",
        "INSTRUCTION_TEMPLATES = {\n",
        "################# PODCAST ##################\n",
        "    \"podcast\": {\n",
        "        \"intro\": \"\"\"Your task is to read out a research paper for a lecture, it should be like an audio book with only one person speaking, it must be word for work without shortening the paper.  You must keep all the text.\n",
        "\n",
        "The exact material covered in the lecture is the provided text.   Don't worry about the formatting issues or any irrelevant information; your goal is to extract the scientific text without changing it. Keep each of the section headers\n",
        "\n",
        "Define all terms used carefully and remove unneeded references such as numbers or anything that isn't part of the main text.\n",
        "\"\"\",\n",
        "        \"text_instructions\": \"\",\n",
        "        \"scratch_pad\": \"\"\"\n",
        "\"\"\",\n",
        "        \"prelude\": \"\"\"\n",
        "\"\"\",\n",
        "        \"dialog\": \"\"\"never shorten the orional text\n",
        "\"\"\",\n",
        "    },\n",
        "\n",
        "}\n",
        "\n",
        "# Function to update instruction fields based on template selection\n",
        "def update_instructions(template):\n",
        "    return (\n",
        "        INSTRUCTION_TEMPLATES[template][\"intro\"],\n",
        "        INSTRUCTION_TEMPLATES[template][\"text_instructions\"],\n",
        "        INSTRUCTION_TEMPLATES[template][\"scratch_pad\"],\n",
        "        INSTRUCTION_TEMPLATES[template][\"prelude\"],\n",
        "        INSTRUCTION_TEMPLATES[template][\"dialog\"]\n",
        "           )\n",
        "\n",
        "import concurrent.futures as cf\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from tempfile import NamedTemporaryFile\n",
        "from typing import List, Literal\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "from loguru import logger\n",
        "from openai import OpenAI\n",
        "from promptic import llm\n",
        "from pydantic import BaseModel, ValidationError\n",
        "from pypdf import PdfReader\n",
        "from tenacity import retry, retry_if_exception_type\n",
        "\n",
        "# Define standard values\n",
        "STANDARD_TEXT_MODELS = [\n",
        "    \"o1-preview-2024-09-12\",\n",
        "    \"o1-preview\",\n",
        "    \"gpt-4o-2024-08-06\",\n",
        "    \"gpt-4o-mini\",\n",
        "    \"o1-mini-2024-09-12\",\n",
        "    \"o1-mini\",\n",
        "    \"chatgpt-4o-latest\",\n",
        "    \"gpt-4-turbo\",\n",
        "    \"openai/custom_model\",\n",
        "]\n",
        "\n",
        "STANDARD_AUDIO_MODELS = [\n",
        "    \"tts-1\",\n",
        "    \"tts-1-hd\",\n",
        "]\n",
        "\n",
        "STANDARD_VOICES = [\n",
        "    \"alloy\",\n",
        "    \"echo\",\n",
        "    \"fable\",\n",
        "    \"onyx\",\n",
        "    \"nova\",\n",
        "    \"shimmer\",\n",
        "]\n",
        "\n",
        "class DialogueItem(BaseModel):\n",
        "    text: str\n",
        "    speaker: Literal[\"speaker-1\", \"speaker-2\"]\n",
        "\n",
        "class Dialogue(BaseModel):\n",
        "    scratchpad: str\n",
        "    dialogue: List[DialogueItem]\n",
        "\n",
        "def get_mp3(text: str, voice: str, audio_model: str, api_key: str = None) -> bytes:\n",
        "    client = OpenAI(\n",
        "        api_key=api_key or os.getenv(\"OPENAI_API_KEY\"),\n",
        "    )\n",
        "\n",
        "    with client.audio.speech.with_streaming_response.create(\n",
        "        model=audio_model,\n",
        "        voice=voice,\n",
        "        input=text,\n",
        "    ) as response:\n",
        "        with io.BytesIO() as file:\n",
        "            for chunk in response.iter_bytes():\n",
        "                file.write(chunk)\n",
        "            return file.getvalue()\n",
        "\n",
        "\n",
        "from functools import wraps\n",
        "\n",
        "def conditional_llm(model, api_base=None, api_key=None):\n",
        "    \"\"\"\n",
        "    Conditionally apply the @llm decorator based on the api_base parameter.\n",
        "    If api_base is provided, it applies the @llm decorator with api_base.\n",
        "    Otherwise, it applies the @llm decorator without api_base.\n",
        "    \"\"\"\n",
        "    def decorator(func):\n",
        "        if api_base:\n",
        "            return llm(model=model, api_base=api_base)(func)\n",
        "        else:\n",
        "            return llm(model=model, api_key=api_key)(func)\n",
        "    return decorator\n",
        "\n",
        "def generate_audio(\n",
        "    files: list,\n",
        "    openai_api_key: str = None,\n",
        "    text_model: str = \"o1-preview-2024-09-12\",\n",
        "    audio_model: str = \"tts-1\",\n",
        "    speaker_1_voice: str = \"alloy\",\n",
        "    speaker_2_voice: str = \"echo\",\n",
        "    api_base: str = None,\n",
        "    intro_instructions: str = '',\n",
        "    text_instructions: str = '',\n",
        "    scratch_pad_instructions: str = '',\n",
        "    prelude_dialog: str = '',\n",
        "    podcast_dialog_instructions: str = '',\n",
        "    edited_transcript: str = None,\n",
        "    user_feedback: str = None,\n",
        "    original_text: str = None,\n",
        "    debug = False,\n",
        ") -> tuple:\n",
        "    # Validate API Key\n",
        "    if not os.getenv(\"OPENAI_API_KEY\") and not openai_api_key:\n",
        "        raise gr.Error(\"OpenAI API key is required\")\n",
        "\n",
        "    combined_text = original_text or \"\"\n",
        "\n",
        "    # If there's no original text, extract it from the uploaded files\n",
        "    if not combined_text:\n",
        "        for file in files:\n",
        "            with Path(file).open(\"rb\") as f:\n",
        "                reader = PdfReader(f)\n",
        "                text = \"\\n\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
        "                combined_text += text + \"\\n\\n\"\n",
        "\n",
        "    # Configure the LLM based on selected model and api_base\n",
        "    @retry(retry=retry_if_exception_type(ValidationError))\n",
        "    @conditional_llm(model=text_model, api_base=api_base, api_key=openai_api_key)\n",
        "    def generate_dialogue(text: str, intro_instructions: str, text_instructions: str, scratch_pad_instructions: str,\n",
        "                          prelude_dialog: str, podcast_dialog_instructions: str,\n",
        "                          edited_transcript: str = None, user_feedback: str = None, ) -> Dialogue:\n",
        "        \"\"\"\n",
        "        {intro_instructions}\n",
        "\n",
        "        Here is the original input text:\n",
        "\n",
        "        <input_text>\n",
        "        {text}\n",
        "        </input_text>\n",
        "\n",
        "        {text_instructions}\n",
        "\n",
        "        <scratchpad>\n",
        "        {scratch_pad_instructions}\n",
        "        </scratchpad>\n",
        "\n",
        "        {prelude_dialog}\n",
        "\n",
        "        <podcast_dialogue>\n",
        "        {podcast_dialog_instructions}\n",
        "        </podcast_dialogue>\n",
        "        {edited_transcript}{user_feedback}\n",
        "        \"\"\"\n",
        "\n",
        "    instruction_improve='Based on the original text, please generate an improved version of the dialogue by incorporating the edits, comments and feedback.'\n",
        "    edited_transcript_processed=\"\\nPreviously generated edited transcript, with specific edits and comments that I want you to carefully address:\\n\"+\"<edited_transcript>\\n\"+edited_transcript+\"</edited_transcript>\" if edited_transcript !=\"\" else \"\"\n",
        "    user_feedback_processed=\"\\nOverall user feedback:\\n\\n\"+user_feedback if user_feedback !=\"\" else \"\"\n",
        "\n",
        "    if edited_transcript_processed.strip()!='' or user_feedback_processed.strip()!='':\n",
        "        user_feedback_processed=\"<requested_improvements>\"+user_feedback_processed+\"\\n\\n\"+instruction_improve+\"</requested_improvements>\"\n",
        "\n",
        "    if debug:\n",
        "        logger.info (edited_transcript_processed)\n",
        "        logger.info (user_feedback_processed)\n",
        "\n",
        "    # Generate the dialogue using the LLM\n",
        "    llm_output = generate_dialogue(\n",
        "        combined_text,\n",
        "        intro_instructions=intro_instructions,\n",
        "        text_instructions=text_instructions,\n",
        "        scratch_pad_instructions=scratch_pad_instructions,\n",
        "        prelude_dialog=prelude_dialog,\n",
        "        podcast_dialog_instructions=podcast_dialog_instructions,\n",
        "        edited_transcript=edited_transcript_processed,\n",
        "        user_feedback=user_feedback_processed\n",
        "    )\n",
        "\n",
        "    # Generate audio from the transcript\n",
        "    audio = b\"\"\n",
        "    transcript = \"\"\n",
        "    characters = 0\n",
        "\n",
        "    with cf.ThreadPoolExecutor() as executor:\n",
        "        futures = []\n",
        "        for line in llm_output.dialogue:\n",
        "            transcript_line = f\"{line.speaker}: {line.text}\"\n",
        "            voice = speaker_1_voice if line.speaker == \"speaker-1\" else speaker_2_voice\n",
        "            future = executor.submit(get_mp3, line.text, voice, audio_model, openai_api_key)\n",
        "            futures.append((future, transcript_line))\n",
        "            characters += len(line.text)\n",
        "\n",
        "        for future, transcript_line in futures:\n",
        "            audio_chunk = future.result()\n",
        "            audio += audio_chunk\n",
        "            transcript += transcript_line + \"\\n\\n\"\n",
        "\n",
        "    logger.info(f\"Generated {characters} characters of audio\")\n",
        "\n",
        "    temporary_directory = \"./gradio_cached_examples/tmp/\"\n",
        "    os.makedirs(temporary_directory, exist_ok=True)\n",
        "\n",
        "    # Use a temporary file -- Gradio's audio component doesn't work with raw bytes in Safari\n",
        "    temporary_file = NamedTemporaryFile(\n",
        "        dir=temporary_directory,\n",
        "        delete=False,\n",
        "        suffix=\".mp3\",\n",
        "    )\n",
        "    temporary_file.write(audio)\n",
        "    temporary_file.close()\n",
        "\n",
        "    # Delete any files in the temp directory that end with .mp3 and are over a day old\n",
        "    for file in glob.glob(f\"{temporary_directory}*.mp3\"):\n",
        "        if os.path.isfile(file) and time.time() - os.path.getmtime(file) > 24 * 60 * 60:\n",
        "            os.remove(file)\n",
        "\n",
        "    return temporary_file.name, transcript, combined_text\n",
        "\n",
        "def validate_and_generate_audio(*args):\n",
        "    files = args[0]\n",
        "    if not files:\n",
        "        return None, None, None, \"Please upload at least one PDF file before generating audio.\"\n",
        "    try:\n",
        "        audio_file, transcript, original_text = generate_audio(*args)\n",
        "        return audio_file, transcript, original_text, None  # Return None as the error when successful\n",
        "    except Exception as e:\n",
        "        # If an error occurs during generation, return None for the outputs and the error message\n",
        "        return None, None, None, str(e)\n",
        "\n",
        "def edit_and_regenerate(edited_transcript, user_feedback, *args):\n",
        "    # Replace the original transcript and feedback in the args with the new ones\n",
        "    #new_args = list(args)\n",
        "    #new_args[-2] = edited_transcript  # Update edited transcript\n",
        "    #new_args[-1] = user_feedback  # Update user feedback\n",
        "    return validate_and_generate_audio(*new_args)\n",
        "\n",
        "# New function to handle user feedback and regeneration\n",
        "def process_feedback_and_regenerate(feedback, *args):\n",
        "    # Add user feedback to the args\n",
        "    new_args = list(args)\n",
        "    new_args.append(feedback)  # Add user feedback as a new argument\n",
        "    return validate_and_generate_audio(*new_args)\n",
        "\n",
        "with gr.Blocks(title=\"PDF to Audio\", css=\"\"\"\n",
        "    #header {\n",
        "        display: flex;\n",
        "        align-items: center;\n",
        "        justify-content: space-between;\n",
        "        padding: 20px;\n",
        "        background-color: transparent;\n",
        "        border-bottom: 1px solid #ddd;\n",
        "    }\n",
        "    #title {\n",
        "        font-size: 24px;\n",
        "        margin: 0;\n",
        "    }\n",
        "    #logo_container {\n",
        "        width: 200px;\n",
        "        height: 200px;\n",
        "        display: flex;\n",
        "        justify-content: center;\n",
        "        align-items: center;\n",
        "    }\n",
        "    #logo_image {\n",
        "        max-width: 100%;\n",
        "        max-height: 100%;\n",
        "        object-fit: contain;\n",
        "    }\n",
        "    #main_container {\n",
        "        margin-top: 20px;\n",
        "    }\n",
        "\"\"\") as demo:\n",
        "\n",
        "    with gr.Row(elem_id=\"header\"):\n",
        "        with gr.Column(scale=4):\n",
        "            gr.Markdown(\"# Convert PDFs into an audio podcast, lecture, summary and others\\n\\nFirst, upload one or more PDFs, select options, then push Generate Audio.\\n\\nYou can also select a variety of custom option and direct the way the result is generated.\", elem_id=\"title\")\n",
        "        with gr.Column(scale=1):\n",
        "            gr.HTML('''\n",
        "                <div id=\"logo_container\">\n",
        "                    <img src=\"https://huggingface.co/spaces/lamm-mit/PDF2Audio/resolve/main/logo.png\" id=\"logo_image\" alt=\"Logo\">\n",
        "                </div>\n",
        "            ''')\n",
        "    #gr.Markdown(\"\")\n",
        "    submit_btn = gr.Button(\"Generate Audio\", elem_id=\"submit_btn\")\n",
        "\n",
        "    with gr.Row(elem_id=\"main_container\"):\n",
        "        with gr.Column(scale=2):\n",
        "            files = gr.Files(label=\"PDFs\", file_types=[\"pdf\"], )\n",
        "\n",
        "            openai_api_key = gr.Textbox(\n",
        "                label=\"OpenAI API Key\",\n",
        "                visible=True,  # Always show the API key field\n",
        "                placeholder=\"Enter your OpenAI API Key here...\",\n",
        "                type=\"password\"  # Hide the API key input\n",
        "            )\n",
        "            text_model = gr.Dropdown(\n",
        "                label=\"Text Generation Model\",\n",
        "                choices=STANDARD_TEXT_MODELS,\n",
        "                value=\"o1-preview-2024-09-12\", #\"gpt-4o-mini\",\n",
        "                info=\"Select the model to generate the dialogue text.\",\n",
        "            )\n",
        "            audio_model = gr.Dropdown(\n",
        "                label=\"Audio Generation Model\",\n",
        "                choices=STANDARD_AUDIO_MODELS,\n",
        "                value=\"tts-1\",\n",
        "                info=\"Select the model to generate the audio.\",\n",
        "            )\n",
        "            speaker_1_voice = gr.Dropdown(\n",
        "                label=\"Speaker 1 Voice\",\n",
        "                choices=STANDARD_VOICES,\n",
        "                value=\"alloy\",\n",
        "                info=\"Select the voice for Speaker 1.\",\n",
        "            )\n",
        "            speaker_2_voice = gr.Dropdown(\n",
        "                label=\"Speaker 2 Voice\",\n",
        "                choices=STANDARD_VOICES,\n",
        "                value=\"echo\",\n",
        "                info=\"Select the voice for Speaker 2.\",\n",
        "            )\n",
        "            api_base = gr.Textbox(\n",
        "                label=\"Custom API Base\",\n",
        "                placeholder=\"Enter custom API base URL if using a custom/local model...\",\n",
        "                info=\"If you are using a custom or local model, provide the API base URL here, e.g.: http://localhost:8080/v1 for llama.cpp REST server.\",\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            template_dropdown = gr.Dropdown(\n",
        "                label=\"Instruction Template\",\n",
        "                choices=list(INSTRUCTION_TEMPLATES.keys()),\n",
        "                value=\"podcast\",\n",
        "                info=\"Select the instruction template to use. You can also edit any of the fields for more tailored results.\",\n",
        "            )\n",
        "            intro_instructions = gr.Textbox(\n",
        "                label=\"Intro Instructions\",\n",
        "                lines=10,\n",
        "                value=INSTRUCTION_TEMPLATES[\"podcast\"][\"intro\"],\n",
        "                info=\"Provide the introductory instructions for generating the dialogue.\",\n",
        "            )\n",
        "            text_instructions = gr.Textbox(\n",
        "                label=\"Standard Text Analysis Instructions\",\n",
        "                lines=10,\n",
        "                placeholder=\"Enter text analysis instructions...\",\n",
        "                value=INSTRUCTION_TEMPLATES[\"podcast\"][\"text_instructions\"],\n",
        "                info=\"Provide the instructions for analyzing the raw data and text.\",\n",
        "            )\n",
        "            scratch_pad_instructions = gr.Textbox(\n",
        "                label=\"Scratch Pad Instructions\",\n",
        "                lines=15,\n",
        "                value=INSTRUCTION_TEMPLATES[\"podcast\"][\"scratch_pad\"],\n",
        "                info=\"Provide the scratch pad instructions for brainstorming presentation/dialogue content.\",\n",
        "            )\n",
        "            prelude_dialog = gr.Textbox(\n",
        "                label=\"Prelude Dialog\",\n",
        "                lines=5,\n",
        "                value=INSTRUCTION_TEMPLATES[\"podcast\"][\"prelude\"],\n",
        "                info=\"Provide the prelude instructions before the presentation/dialogue is developed.\",\n",
        "            )\n",
        "            podcast_dialog_instructions = gr.Textbox(\n",
        "                label=\"Podcast Dialog Instructions\",\n",
        "                lines=20,\n",
        "                value=INSTRUCTION_TEMPLATES[\"podcast\"][\"dialog\"],\n",
        "                info=\"Provide the instructions for generating the presentation or podcast dialogue.\",\n",
        "            )\n",
        "\n",
        "    audio_output = gr.Audio(label=\"Audio\", format=\"mp3\", interactive=False, autoplay=False)\n",
        "    transcript_output = gr.Textbox(label=\"Transcript\", lines=20, show_copy_button=True)\n",
        "    original_text_output = gr.Textbox(label=\"Original Text\", lines=10, visible=False)\n",
        "    error_output = gr.Textbox(visible=False)  # Hidden textbox to store error message\n",
        "\n",
        "    use_edited_transcript = gr.Checkbox(label=\"Use Edited Transcript (check if you want to make edits to the initially generated transcript)\", value=False)\n",
        "    edited_transcript = gr.Textbox(label=\"Edit Transcript Here. E.g., mark edits in the text with clear instructions. E.g., '[ADD DEFINITION OF MATERIOMICS]'.\", lines=20, visible=False,\n",
        "                                   show_copy_button=True, interactive=False)\n",
        "\n",
        "    user_feedback = gr.Textbox(label=\"Provide Feedback or Notes\", lines=10, #placeholder=\"Enter your feedback or notes here...\"\n",
        "                              )\n",
        "    regenerate_btn = gr.Button(\"Regenerate Audio with Edits and Feedback\")\n",
        "    # Function to update the interactive state of edited_transcript\n",
        "    def update_edit_box(checkbox_value):\n",
        "        return gr.update(interactive=checkbox_value, lines=20 if checkbox_value else 20, visible=True if checkbox_value else False)\n",
        "\n",
        "    # Update the interactive state of edited_transcript when the checkbox is toggled\n",
        "    use_edited_transcript.change(\n",
        "        fn=update_edit_box,\n",
        "        inputs=[use_edited_transcript],\n",
        "        outputs=[edited_transcript]\n",
        "    )\n",
        "    # Update instruction fields when template is changed\n",
        "    template_dropdown.change(\n",
        "        fn=update_instructions,\n",
        "        inputs=[template_dropdown],\n",
        "        outputs=[intro_instructions, text_instructions, scratch_pad_instructions, prelude_dialog, podcast_dialog_instructions]\n",
        "    )\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=validate_and_generate_audio,\n",
        "        inputs=[\n",
        "            files, openai_api_key, text_model, audio_model,\n",
        "            speaker_1_voice, speaker_2_voice, api_base,\n",
        "            intro_instructions, text_instructions, scratch_pad_instructions,\n",
        "            prelude_dialog, podcast_dialog_instructions,\n",
        "            edited_transcript,  # placeholder for edited_transcript\n",
        "            user_feedback,  # placeholder for user_feedback\n",
        "        ],\n",
        "        outputs=[audio_output, transcript_output, original_text_output, error_output]\n",
        "    ).then(\n",
        "        fn=lambda audio, transcript, original_text, error: (\n",
        "            transcript if transcript else \"\",\n",
        "            error if error else None\n",
        "        ),\n",
        "        inputs=[audio_output, transcript_output, original_text_output, error_output],\n",
        "        outputs=[edited_transcript, error_output]\n",
        "    ).then(\n",
        "        fn=lambda error: gr.Warning(error) if error else None,\n",
        "        inputs=[error_output],\n",
        "        outputs=[]\n",
        "    )\n",
        "\n",
        "    regenerate_btn.click(\n",
        "        fn=lambda use_edit, edit, *args: validate_and_generate_audio(\n",
        "            *args[:12],  # All inputs up to podcast_dialog_instructions\n",
        "            edit if use_edit else \"\",  # Use edited transcript if checkbox is checked, otherwise empty string\n",
        "            *args[12:]  # user_feedback and original_text_output\n",
        "        ),\n",
        "        inputs=[\n",
        "            use_edited_transcript, edited_transcript,\n",
        "            files, openai_api_key, text_model, audio_model,\n",
        "            speaker_1_voice, speaker_2_voice, api_base,\n",
        "            intro_instructions, text_instructions, scratch_pad_instructions,\n",
        "            prelude_dialog, podcast_dialog_instructions,\n",
        "            user_feedback, original_text_output\n",
        "        ],\n",
        "        outputs=[audio_output, transcript_output, original_text_output, error_output]\n",
        "    ).then(\n",
        "        fn=lambda audio, transcript, original_text, error: (\n",
        "            transcript if transcript else \"\",\n",
        "            error if error else None\n",
        "        ),\n",
        "        inputs=[audio_output, transcript_output, original_text_output, error_output],\n",
        "        outputs=[edited_transcript, error_output]\n",
        "    ).then(\n",
        "        fn=lambda error: gr.Warning(error) if error else None,\n",
        "        inputs=[error_output],\n",
        "        outputs=[]\n",
        "    )\n",
        "\n",
        "    # Add README content at the bottom\n",
        "    gr.Markdown(\"---\")  # Horizontal line to separate the interface from README\n",
        "    gr.Markdown(read_readme())\n",
        "\n",
        "# Enable queueing for better performance\n",
        "demo.queue(max_size=20, default_concurrency_limit=32)\n",
        "\n",
        "# Launch the Gradio app\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvlp5EvQ2ApM"
      },
      "source": [
        "## Credits\n",
        "\n",
        "This project was inspired by and based on the code available at [https://github.com/knowsuchagency/pdf-to-podcast](https://github.com/knowsuchagency/pdf-to-podcast) and [https://github.com/knowsuchagency/promptic](https://github.com/knowsuchagency/promptic).\n",
        "\n",
        "```bibtex\n",
        "@article{ghafarollahi2024sciagentsautomatingscientificdiscovery,\n",
        "    title={SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning},\n",
        "    author={Alireza Ghafarollahi and Markus J. Buehler},\n",
        "    year={2024},\n",
        "    eprint={2409.05556},\n",
        "    archivePrefix={arXiv},\n",
        "    primaryClass={cs.AI},\n",
        "    url={https://arxiv.org/abs/2409.05556},\n",
        "}\n",
        "@article{buehler2024graphreasoning,\n",
        "    title={Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning},\n",
        "    author={Markus J. Buehler},\n",
        "    journal={Machine Learning: Science and Technology},\n",
        "    year={2024},\n",
        "    url={http://iopscience.iop.org/article/10.1088/2632-2153/ad7228},\n",
        "}\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}